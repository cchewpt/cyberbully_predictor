{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Set the starting index\\nstart_index = 0\\n\\n# Create an empty list to store the selected rows\\nselected_rows_list = []\\n\\n# Read 1 row, skip 100, and repeat until 200 rows are read\\nfor _ in range(200):  # This loop will run 200 times\\n    # Select the current row using iloc\\n    selected_row = tweet.iloc[start_index:start_index + 1]\\n\\n    # Check if there are any rows left to read\\n    if not selected_row.empty:\\n        # Append the current row to the list as a dictionary\\n        selected_rows_list.append(selected_row.to_dict(orient=\\'records\\')[0])\\n\\n    # Update the starting index for the next iteration\\n    start_index += 101  # Skip 100 rows and read the next one\\n\\n# Convert the list of dictionaries to a DataFrame\\nselected_rows_df = pd.DataFrame(selected_rows_list)\\n\\n# Save the selected rows to a new CSV file\\nselected_rows_df.to_csv(\"selected_rows.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tweet = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "# for index, row in tweet.iterrows():\n",
    "#     print(row[\"tweet_text\"], row[\"cyberbullying_type\"])\n",
    "\n",
    "# tweet = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "\n",
    "# # Set the starting index\n",
    "# start_index = 0\n",
    "\n",
    "# # Read 1 row, skip 100, and repeat until 200 rows are read\n",
    "# for _ in range(200):  # This loop will run 200 times\n",
    "#     # Select the current row using iloc\n",
    "#     selected_row = tweet.iloc[start_index:start_index + 1]\n",
    "\n",
    "#     # Check if there are any rows left to read\n",
    "#     if not selected_row.empty:\n",
    "#         # Print the current row\n",
    "#         row = selected_row.iloc[0]\n",
    "#         print(row[\"tweet_text\"], row[\"cyberbullying_type\"])\n",
    "\n",
    "#     # Update the starting index for the next iteration\n",
    "#     start_index += 101  # Skip 100 rows and read the next one\n",
    "#---------------------------------------------------------------\n",
    "tweet = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "'''\n",
    "# Set the starting index\n",
    "start_index = 0\n",
    "\n",
    "# Create an empty list to store the selected rows\n",
    "selected_rows_list = []\n",
    "\n",
    "# Read 1 row, skip 100, and repeat until 200 rows are read\n",
    "for _ in range(200):  # This loop will run 200 times\n",
    "    # Select the current row using iloc\n",
    "    selected_row = tweet.iloc[start_index:start_index + 1]\n",
    "\n",
    "    # Check if there are any rows left to read\n",
    "    if not selected_row.empty:\n",
    "        # Append the current row to the list as a dictionary\n",
    "        selected_rows_list.append(selected_row.to_dict(orient='records')[0])\n",
    "\n",
    "    # Update the starting index for the next iteration\n",
    "    start_index += 101  # Skip 100 rows and read the next one\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "selected_rows_df = pd.DataFrame(selected_rows_list)\n",
    "\n",
    "# Save the selected rows to a new CSV file\n",
    "selected_rows_df.to_csv(\"selected_rows.csv\", index=False)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "cut = df.dropna(subset=['cyberbullying_type'])\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text cyberbullying_type  \\\n",
      "0                                word food crapilici  not_cyberbullying   \n",
      "1                                              white  not_cyberbullying   \n",
      "2                     classi whore red velvet cupcak  not_cyberbullying   \n",
      "3  meh p thank head concern anoth angri dude twitter  not_cyberbullying   \n",
      "4  isi account pretend kurdish account like islam...  not_cyberbullying   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0                            [word, food, crapilici]  \n",
      "1                                            [white]  \n",
      "2               [classi, whore, red, velvet, cupcak]  \n",
      "3  [meh, p, thank, head, concern, anoth, angri, d...  \n",
      "4  [isi, account, pretend, kurdish, account, like...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"cyberbullying_tweets.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your text column is named \"tweet_text\" and the label column is \"cyberbullying_type\"\n",
    "text_column_name = \"tweet_text\"\n",
    "label_column_name = \"cyberbullying_type\"\n",
    "\n",
    "# Replace \"›\" with an empty string\n",
    "df[text_column_name] = df[text_column_name].str.replace('›', '')\n",
    "\n",
    "pattern = r'@([A-Za-z0-9_]+)'\n",
    "\n",
    "# Function to remove Twitter usernames from text\n",
    "def remove_twitter_username(text):\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df[text_column_name] = df[text_column_name].apply(remove_twitter_username)\n",
    "\n",
    "pattern2 = r'#([A-Za-z0-9_]+)'\n",
    "\n",
    "# Function to remove Twitter usernames from text\n",
    "def remove_twitter_username(text):\n",
    "    return re.sub(pattern2, '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df[text_column_name] = df[text_column_name].apply(remove_twitter_username)\n",
    "\n",
    "# Define a regular expression pattern to match web domains\n",
    "pattern3 = r'\\b(?:https?://)?(?:www\\.)?([a-zA-Z0-9-]+(?:\\.[a-zA-Z]{2,})+)\\b'\n",
    "\n",
    "# Function to remove web domains from text\n",
    "def remove_web_domains(text):\n",
    "    return re.sub(pattern3, '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "df[text_column_name] = df[text_column_name].apply(remove_web_domains)\n",
    "\n",
    "# Lowercasing\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Removing Punctuation and Special Characters\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [word.translate(str.maketrans(\"\", \"\", string.punctuation)) for word in x])\n",
    "\n",
    "# Removing Stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# Remove mentions (starting with '@') and content after it until a space is found\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [word if not (word.startswith('@') and len(word) > 1) else '' for word in x])\n",
    "\n",
    "# Remove empty strings\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [word for word in x if word != ''])\n",
    "\n",
    "# Remove URLs\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", word, flags=re.MULTILINE) for word in x])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Remove extra spaces\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "# Remove emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "df[text_column_name] = df[text_column_name].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "# Replace \"›\" with an empty string\n",
    "df[text_column_name] = df[text_column_name].str.replace('›', '')\n",
    "df[text_column_name] = df[text_column_name].str.replace('rt', '')\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize the 'text' column\n",
    "df['tokenized_text'] = df[text_column_name].apply(tokenize_text)\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "output_file_path = \"preprocessed_data.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
